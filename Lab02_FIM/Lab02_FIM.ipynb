{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 20120547\n",
    "- Student name: Võ Thành Phong\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    ret=list(set(a) | set(b))\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        support=0\n",
    "        for i in self.data.keys():\n",
    "            if len(set(itemset)) == len(set(itemset).intersection(self.data[i])):\n",
    "                support+=1\n",
    "        return support/len(list(self.data.keys()))\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        if k==1:\n",
    "            minSup=self.minSup\n",
    "        elif k>1:\n",
    "            minSup=self.minSup/len(list(self.data.keys()))\n",
    "        for i in search_space.keys():\n",
    "            if search_space[i]['support']>=minSup:\n",
    "                self.L[k].append(search_space[i]['itemset'])\n",
    "            else:\n",
    "                search_space[i]['pruned']=True\n",
    "        if self.L[k]==[]:\n",
    "            del self.L[k]\n",
    "\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                newset=joinset(search_space[a]['itemset'],search_space[b]['itemset'])\n",
    "                sp_newset=self.computeItemsetSupport(newset)\n",
    "                # Second add newset to search_space\n",
    "                search_space[a][b]={'itemset':list(newset),'pruned':False,'support':sp_newset}\n",
    "                sub_search_space[b]=search_space[a][b]\n",
    "                sub_tree[b]={}\n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [[48], [56], [66], [34], [62], [7], [36], [60], [40], [29], [52], [58]], 2: [[48, 52], [48, 58], [56, 29], [56, 52], [56, 58], [66, 60], [66, 29], [66, 52], [66, 58], [40, 34], [34, 29], [34, 52], [34, 58], [60, 62], [40, 62], [29, 62], [52, 62], [58, 62], [60, 7], [40, 7], [29, 7], [52, 7], [58, 7], [36, 60], [40, 36], [36, 29], [36, 52], [58, 36], [40, 60], [60, 29], [60, 52], [58, 60], [40, 29], [40, 52], [40, 58], [52, 29], [58, 29], [58, 52]], 3: [[48, 58, 52], [56, 52, 29], [56, 58, 29], [56, 58, 52], [66, 60, 29], [66, 60, 52], [66, 60, 58], [66, 52, 29], [66, 58, 29], [66, 52, 58], [40, 34, 29], [40, 34, 52], [40, 34, 58], [34, 52, 29], [34, 58, 29], [34, 52, 58], [60, 29, 62], [60, 62, 52], [58, 60, 62], [40, 29, 62], [40, 52, 62], [40, 58, 62], [52, 29, 62], [58, 29, 62], [58, 52, 62], [40, 60, 7], [60, 29, 7], [60, 52, 7], [58, 60, 7], [40, 29, 7], [40, 52, 7], [40, 58, 7], [52, 29, 7], [58, 29, 7], [58, 52, 7], [40, 36, 60], [36, 29, 60], [36, 60, 52], [58, 36, 60], [40, 36, 29], [40, 36, 52], [40, 58, 36], [36, 29, 52], [58, 36, 29], [58, 36, 52], [40, 60, 29], [40, 60, 52], [40, 58, 60], [60, 29, 52], [58, 60, 29], [58, 60, 52], [40, 52, 29], [40, 58, 29], [40, 58, 52], [58, 52, 29]], 4: [[52, 56, 58, 29], [66, 52, 60, 29], [66, 58, 60, 29], [66, 52, 58, 60], [66, 52, 58, 29], [34, 52, 40, 29], [34, 40, 58, 29], [34, 52, 40, 58], [34, 52, 58, 29], [58, 60, 29, 62], [52, 58, 60, 62], [52, 40, 29, 62], [40, 58, 29, 62], [52, 40, 58, 62], [52, 58, 29, 62], [7, 40, 58, 60], [52, 7, 60, 29], [7, 58, 60, 29], [52, 7, 58, 60], [52, 7, 40, 29], [7, 40, 58, 29], [52, 7, 40, 58], [52, 7, 58, 29], [36, 40, 60, 29], [36, 52, 40, 60], [36, 40, 58, 60], [36, 52, 60, 29], [36, 58, 60, 29], [36, 52, 58, 60], [36, 52, 40, 29], [36, 40, 58, 29], [36, 52, 40, 58], [36, 52, 58, 29], [52, 40, 60, 29], [40, 58, 60, 29], [52, 40, 58, 60], [52, 58, 60, 29], [52, 40, 58, 29]], 5: [[66, 52, 58, 60, 29], [34, 40, 52, 58, 29], [40, 52, 58, 29, 62], [7, 52, 58, 60, 29], [7, 40, 52, 58, 29], [36, 40, 52, 60, 29], [36, 40, 58, 60, 29], [36, 40, 52, 58, 60], [36, 52, 58, 60, 29], [36, 40, 52, 58, 29], [40, 52, 58, 60, 29]], 6: [[36, 40, 52, 58, 60, 29]]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "- Bởi vì, thứ nhất là do các bước đọc dữ liệu từ hàm readData không thể biết trước được số lượng transactions có trong dữ liệu. Bộ dữ liệu được đọc bằng cách duyệt qua lần lượt từng transaction để đọc tất cả item vào đối tượng s, nếu duyệt qua toàn bộ để biết được số lượng transaction sẽ lập code làm tăng thời gian không đáng có.\n",
    "- Hơn nữa việc tính support cho itemset sẽ được lặp lại nhiều lần về sau cho các L-itemsets khác (với L>1). Do đó viết một hàm tính support trong class TP một cách phù hợp sẽ hữu ích cho các bước sau.\n",
    "\n",
    "**why should we do sort**\n",
    "- Việc sắp xếp các item trong self.s theo thứ tự tăng dần của support count sẽ giúp cho việc nếu xác định được một itemset là không phổ biến thì nó sẽ có số lượng item là nhỏ nhất.\n",
    "- Ví dụ ta có các cặp item:support count trong self.s được xếp lộn xộn như sau: {3:3, 1:1, 2:3} với min support count là 2 thì theo thuật toán Tree Projection được cài đặt bên trên, itemset [3] thỏa là tập phổ biến nên sẽ tiếp tục tạo không gian tìm kiếm cho trường hợp k=2 bắt đầu từ 3 và [3,1] và [3,2] sẽ được thêm vào search space rồi mới thực hiện tỉa. Trong khi có thể bỏ qua tất cả lần duyệt các itemset có chứa item 1 nếu xét item 1 đầu tiên ngay từ đầu.\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "- Set trong Python là kiểu dữ liệu tập hợp không có thứ tự, có thể thay đổi và không có phần tử trùng lặp. Set được ký hiệu bởi cặp dấu ngoặc nhọn {}. Các phần tử của set sẽ được đặt trong cặp ngoặc nhọn.\n",
    "- Set không có tính thứ tự do việc cài đặt set là bằng bảng băm, chúng ta sẽ không biết thứ tự lưu các phần tử vào set như thế nào mà tùy thuộc vào cách băm. Do việc cài đặt bàng bảng băm nên set có các ưu điểm chính như sau:\n",
    "  + Điều này cho phép các phép toán trên set được thực hiện với độ phức tạp thời gian trung bình là O(1). Khi kiểm tra xem một phần tử có trong set hay không Python chỉ cần tính toán mã băm của phần tử và kiểm tra xem phần tử có trong bảng băm hay không.\n",
    "  + Set không chứa các phần tử trùng lặp, nên rất dễ dàng để thực hiện phép hội.\n",
    "  + Set hỗ trợ mạnh về các phép toán liên quan đến tập hợp như giao, hợp, hiệu, .... Do đó việc kiểm tra phần tử có trong set trở nên rất dễ dàng.\n",
    "- Vậy lợi ích của việc dùng set trong khi cài đặt thuật toán Tree Projection:\n",
    "  + Thực hiện việc hợp(join) 2 itemset trong hàm joinset sẽ loại bỏ được việc trùng lặp phần tử ngay lập tức mà không cần code quá phức tạp.\n",
    "  + Khi tính support cho các L-itemsets (L>1) trong hàm computeItemsetSupport, ta kiểm tra các item của một itemset có cùng xuất hiện trong một transaction hay không rất dễ dàng qua phép toán giao (intersection) trên set.\n",
    "  + Khi bộ dữ liệu rất lớn thì làm việc trên set giúp tăng tốc độ hơn rất nhiều so với trên list.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "- Việc dùng pruned = True/False để cho biết itemset có là phổ biến hay không mà không xóa trực tiếp trong search_space và tree là do thuật toán được cài đặt tìm kiếm các itemset phổ biến một cách đệ qui bắt đầu từ không gian tìm kiếm cho k=1 và tăng dần lên k=2, 3, 4, ....\n",
    "- Và ở không gian tìm kiếm cho k+1 thì tree và search_space sẽ lại là sub_tree và sub_searchspace với các bộ key-value hoàn toàn mới được kế thừa từ việc kết hợp các itemset ở không gian k. Do đó nếu có xóa trực tiếp thì cuối cùng nếu tất cả 1-itemset ban đầu đều là tập phổ biến thì search_space và tree vẫn chứa đầy đủ các nhanh và lúc này không có cách để nhận biết đâu là tập không phổ biến và nhánh nên bị tỉa.\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "\n",
    "**1. Về cấu trúc dữ liệu:**\n",
    "  - Apriori dùng cấu trúc dạng tập hợp (như list, bảng) để lưu các tập ứng viên C và các tập phổ biến L.\n",
    "  - Tree Projection sẽ lưu các itemset trong các node của cấu trúc cây.\n",
    "**2. Cách tìm tập phổ biến:**\n",
    "  - Apriori sẽ tạo tập ${C}_{k+1}$ bằng cách kết hợp tất cả các itemset trong ${L}_{k}$ lại với nhau. Rồi sau đó duyệt lần lượt qua hết ${C}_{k+1}$ để tìm những itemset thỏa min support và thêm vào ${L}_{k+1}$.\n",
    "  - Tree Projection sẽ lưu ở mức 1 của cây các 1-itemset ứng viên và sau đó loại ngay lập tức các itemset ứng viên không là tập phổ biến, sau đó từ các 1-itemset phổ biến còn lại tạo tiếp **theo chiều sâu** các L-itemset ứng viên (L>1) ở các mức cao hơn của cây, và vẫn tiến hành loại ngay những itemset nào không phổ biến.\n",
    "  - Ví dụ ta có các item ban đầu của database là 1, 2, 3, 4. **Giả sử:** itemset [1] sẽ không thỏa min support, các itemset [2], [3], [4] và tất cả các itemset khác được tạo ra sau này đều sẽ thỏa min support, ta có hình ảnh như sau:\n",
    "  ![img](https://res.cloudinary.com/vtphong/image/upload/v1681221934/data-mining/image1.png)\n",
    "  - Đầu tiên là xóa itemset [1], sau đó tạo các itemset khác theo chiều các mũi tên màu đỏ và cuối cùng mới đến tạo itemset [3, 4].\n",
    "**3. Về hiệu suất:**\n",
    "  - Thuật toán Tree Projection thường cho hiệu suất tốt hơn về mặt tốc độ so với thuật toán Apriori, đặc biệt là khi xử lý các tập dữ liệu lớn và các tập phổ biến có kích thước lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trước tiên chuyển file 'churn.txt' sang flat file với định dạng là txt bằng hàm convertDataset tự định nghĩa sau đây.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mô tả ý tưởng tạo data:**\n",
    "- Dòng đầu trong file churn chứa tên thuộc tính, các dòng sau chứa từng giá trị cho mỗi thuộc tính của mỗi khách hàng.\n",
    "- Do đó ta có thể tạo data có dạng như sau:\n",
    "  + Ví dụ mỗi hàng của file chứa data đã chuyển đổi sẽ tương ứng là thông tin của mỗi khách hàng trong file churn.txt. Khi đó file data đã chuyển đổi sẽ có dạng:\n",
    "  \n",
    "<thuộc tính 1>:<giá trị 1_1>;<thuộc tính 2>:<giá trị 1_2>;...;<thuộc tính n>:<giá trị 1_n>\n",
    "  \n",
    "<thuộc tính 1>:<giá trị 2_1>;<thuộc tính 2>:<giá trị 2_2>;...;<thuộc tính n>:<giá trị 2_n>\n",
    "  \n",
    "...\n",
    "  \n",
    "<thuộc tính 1>:<giá trị m_1>;<thuộc tính 2>;<giá trị m_2>;...;<thuộc tính n>:<giá trị m_n>\n",
    "  \n",
    "  + **Với n là số thuộc tính ban đầu trong file churn.txt, m là số khách hàng ban đầu trong file churn.txt**.\n",
    "  + **Đặt tên là 'data.txt'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataset(path):\n",
    "    columns_name=[\"State\",\"Account Length\",\"Area Code\",\"Phone\",\"Int'l Plan\",\"VMail Plan\",\"VMail Message\",\n",
    "                  \"Day Mins\",\"Day Calls\",\"Day Charge\",\"Eve Mins\",\"Eve Calls\",\"Eve Charge\",\"Night Mins\",\n",
    "                  \"Night Calls\",\"Night Charge\",\"Intl Mins\",\"Intl Calls\",\"Intl Charge\",\"CustServ Calls\",\"Churn?\"]\n",
    "    f = open(path, 'r')\n",
    "    writer = open('data.txt','w')\n",
    "    temp=f.readline() #loại bỏ dòng đầu là các thuộc tính\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "        data[-1]=data[-1].replace('.','')\n",
    "        for i in range(len(columns_name)):\n",
    "            data[i]=columns_name[i]+':'+data[i]\n",
    "        row=';'.join(data)\n",
    "        writer.write(row)\n",
    "    f.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertDataset('churn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mô tả ý tưởng phân tích dữ liệu churn:**\n",
    "- Churn là dữ liệu khách hàng rời bỏ dịch vụ. Với các thông tin liên quan thì khách hàng có rời bỏ dịch vụ hay không?\n",
    "- Vậy ta có thể nghĩ đến việc tìm các tập phổ biến có chứa yếu tố churn, sau đó tím ra các luật kết hợp mà yếu tố churn nằm ở vế phải thỏa một min confidence nào đó, tất nhiên những yếu tố ở vế trái của luật sẽ là những yếu tố ảnh hưởng đến việc khách hàng có rời bỏ dịch vụ hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tận dụng lại hàm readData, và lớp TP để tìm tập phổ biến.** Nhưng cần định nghĩa lại một chút hàm readDataset để có thể lưu các itemset kiểu string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            itemset=set(line.split(';')) \n",
    "            for item in itemset:  \n",
    "                s[item]+=1    \n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnAnalysis:\n",
    "    def __init__(self, data=None, s=None, L=None, attribute=None, minConf=None):\n",
    "        self.data=data\n",
    "        self.s=s\n",
    "        self.L=L\n",
    "        self.attribute=attribute\n",
    "        self.minConf=minConf\n",
    "        self.filtered_L={}\n",
    "        self.AscRules=[]\n",
    "        \n",
    "    def joinset(a, b):\n",
    "        ret=list(set(a) | set(b))\n",
    "        return ret\n",
    "    \n",
    "    def filterFrequentItemsets(self):\n",
    "        L2={}\n",
    "        for k in self.L.keys():\n",
    "            if k==1:\n",
    "                continue\n",
    "            L2[k]=[]\n",
    "            for i in range(len(self.L[k])):\n",
    "                L2[k].append([])\n",
    "                check_exist=False\n",
    "                for item in self.L[k][i]:\n",
    "                    if self.attribute in item:\n",
    "                        check_exist=True\n",
    "                if check_exist==True:\n",
    "                    for item in self.L[k][i]:\n",
    "                        L2[k][i].append(item)\n",
    "            for i in L2[k]:\n",
    "                if len(i)==0:\n",
    "                    L2[k].remove(i)\n",
    "            if len(L2[k])==0:\n",
    "                del L2[k]\n",
    "        return L2\n",
    "    \n",
    "    def computeItemsetSupport(self,itemset):\n",
    "        support=0\n",
    "        for i in self.data.keys():\n",
    "            if len(set(itemset)) == len(set(itemset).intersection(self.data[i])):\n",
    "                support+=1\n",
    "        return support/len(self.data.keys())\n",
    "    \n",
    "    def computeRuleSupport(self, leftSide, rightSide):\n",
    "        return self.computeItemsetSupport(joinset(leftSide, rightSide))\n",
    "    \n",
    "    def computeRuleConfidence(self, leftSide, rightSide):\n",
    "        return self.computeItemsetSupport(joinset(leftSide, rightSide))/self.computeItemsetSupport(leftSide)\n",
    "    \n",
    "    def generateARule(self,itemset):\n",
    "        itemset_copy=itemset.copy()\n",
    "        leftSide, rightSide=[], []\n",
    "        rule={}\n",
    "        for i in itemset:\n",
    "            if self.attribute in i:\n",
    "                rightSide.append(i)\n",
    "                itemset_copy.remove(i)\n",
    "                leftSide=itemset_copy\n",
    "                break\n",
    "        rule[tuple(leftSide)]={}\n",
    "        rule[tuple(leftSide)]['result']=rightSide\n",
    "        rule[tuple(leftSide)]['support']=self.computeRuleSupport(leftSide, rightSide)\n",
    "        rule[tuple(leftSide)]['confidence']=self.computeRuleConfidence(leftSide, rightSide)\n",
    "        return rule\n",
    "    \n",
    "    def generateAllRules(self):\n",
    "        self.filtered_L=self.filterFrequentItemsets()\n",
    "        for k in self.filtered_L.keys():\n",
    "            for i in self.filtered_L[k]:\n",
    "                rule=self.generateARule(i)\n",
    "                if list(rule.values())[0]['confidence']>=self.minConf:\n",
    "                    self.AscRules.append(rule)\n",
    "        return self.AscRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2, s2=readDataset('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSup=int(0.5*len(list(data2.keys()))) #minSup bằng 50% số lượng transactions\n",
    "frequentItemsets=TP(data=data2,s=s2, minSup=minSup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules=ChurnAnalysis(data=data2,s=s2,L=frequentItemsets.miningResults(),attribute='Churn?',minConf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('VMail Plan:no',): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {('VMail Message:0',): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {(\"Int'l Plan:no\",): {'result': ['Churn?:False'],\n",
       "   'support': 0.7992799279927992,\n",
       "   'confidence': 0.8850498338870432}},\n",
       " {('VMail Plan:no', 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {('VMail Plan:no', \"Int'l Plan:no\"): {'result': ['Churn?:False'],\n",
       "   'support': 0.5634563456345635,\n",
       "   'confidence': 0.861467889908257}},\n",
       " {(\"Int'l Plan:no\", 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.5634563456345635,\n",
       "   'confidence': 0.861467889908257}},\n",
       " {('VMail Plan:no',\n",
       "   \"Int'l Plan:no\",\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.5634563456345635, 'confidence': 0.861467889908257}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.generateAllRules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Em tạo một lớp tên là ChurnAnalysis để phân tích tập churn như sau:\n",
    "  + Phương thức filterFrequentItemsets sẽ lọc ra các tập L-itemsets (L>=2) là tập hợp những tập phổ biến chứa yếu tố churn nhưng có từ 2 phần tử trở lên (do để tạo luật từ itemset phải có từ 2 phần tử trở lên trong itemset).\n",
    "  + Phương thức computeRuleConfidence tính confidence cho luật.\n",
    "  + Phương thức generateAllRules dùng để sinh tất cả các luật mà vế phải là yếu tố churn thỏa min confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để không mất tính tổng quát cho lớp và các phương thức, có thể đổi yếu tố churn thành yếu tố khác và min confidence là một giá trị khác để truyền vào lớp và sinh các luật liên quan.\n",
    "- Có thể thấy list trên chứa các dictionary với mỗi dictionary là một luật:\n",
    "  + Key sẽ là vế trái của luật.\n",
    "  + 'result' sẽ là vế phải của luật.\n",
    "  + support là support của luật.\n",
    "  + confidence là confidence của luật."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Có thể thấy trong dataset này với min support bằng 50% thì không có tập phổ biến nào chứa churn=True (khách hàng rời bỏ dịch vụ).\n",
    "- Có nhiều tập phổ biến chứa churn=False, vậy ta sẽ phân tích theo hướng kết hợp những yếu tố nào để tăng khả năng giữ lại khách hàng. Và để làm được điều này thì những luật ở trên là sự gợi ý tốt để tham khảo.\n",
    "- **Để xem tất cả các tập phổ biến khi chưa lọc theo thuộc tính được chọn, có thể dùng thuộc tính .L**.\n",
    "- **Để xem tất cả các tập phổ biến khi đã lọc theo thuộc tính được chọn, có thể dùng thuộc tính .filtered_L**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tất cả các tập phổ biến với min support đã cho:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [['VMail Plan:no'],\n",
       "  ['VMail Message:0'],\n",
       "  ['Churn?:False'],\n",
       "  [\"Int'l Plan:no\"]],\n",
       " 2: [['VMail Plan:no', 'VMail Message:0'],\n",
       "  ['VMail Plan:no', 'Churn?:False'],\n",
       "  ['VMail Plan:no', \"Int'l Plan:no\"],\n",
       "  ['Churn?:False', 'VMail Message:0'],\n",
       "  [\"Int'l Plan:no\", 'VMail Message:0'],\n",
       "  ['Churn?:False', \"Int'l Plan:no\"]],\n",
       " 3: [['VMail Plan:no', 'Churn?:False', 'VMail Message:0'],\n",
       "  ['VMail Plan:no', \"Int'l Plan:no\", 'VMail Message:0'],\n",
       "  ['VMail Plan:no', 'Churn?:False', \"Int'l Plan:no\"],\n",
       "  ['Churn?:False', \"Int'l Plan:no\", 'VMail Message:0']],\n",
       " 4: [['Churn?:False', 'VMail Plan:no', \"Int'l Plan:no\", 'VMail Message:0']]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Tất cả các tập phổ biến với min support đã cho:\\n')\n",
    "rules.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nếu trên phương diện là chủ dịch vụ, để chắc chắn là không có yếu tố ảnh hưởng nào làm cho churn=true đáng lo ngại xảy ra, em sẽ xem xét tập dữ liệu với một min support rất thấp hơn (20% cũng đã đủ làm em lo lắng 😁)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSup2=int(0.2*len(list(data2.keys()))) #minSup bằng 20% số lượng transactions\n",
    "frequentItemsets2=TP(data=data2,s=s2, minSup=minSup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules2=ChurnAnalysis(data=data2,s=s2,L=frequentItemsets2.miningResults(),attribute='Churn?',minConf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('CustServ Calls:2',): {'result': ['Churn?:False'],\n",
       "   'support': 0.20162016201620162,\n",
       "   'confidence': 0.8853754940711462}},\n",
       " {('Area Code:408',): {'result': ['Churn?:False'],\n",
       "   'support': 0.2148214821482148,\n",
       "   'confidence': 0.8544152744630071}},\n",
       " {('Area Code:510',): {'result': ['Churn?:False'],\n",
       "   'support': 0.2145214521452145,\n",
       "   'confidence': 0.8511904761904762}},\n",
       " {('VMail Plan:yes',): {'result': ['Churn?:False'],\n",
       "   'support': 0.2526252625262526,\n",
       "   'confidence': 0.9132321041214749}},\n",
       " {('CustServ Calls:1',): {'result': ['Churn?:False'],\n",
       "   'support': 0.3177317731773177,\n",
       "   'confidence': 0.8966977138018628}},\n",
       " {('Area Code:415',): {'result': ['Churn?:False'],\n",
       "   'support': 0.42574257425742573,\n",
       "   'confidence': 0.8574018126888218}},\n",
       " {(): {'result': [], 'support': 1.0, 'confidence': 1.0}},\n",
       " {('VMail Plan:no',): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {(): {'result': [], 'support': 1.0, 'confidence': 1.0}},\n",
       " {('VMail Message:0',): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {(): {'result': [], 'support': 1.0, 'confidence': 1.0}},\n",
       " {(\"Int'l Plan:no\",): {'result': ['Churn?:False'],\n",
       "   'support': 0.7992799279927992,\n",
       "   'confidence': 0.8850498338870432}},\n",
       " {(\"Int'l Plan:no\", 'Area Code:408'): {'result': ['Churn?:False'],\n",
       "   'support': 0.20312031203120312,\n",
       "   'confidence': 0.8826597131681878}},\n",
       " {('VMail Plan:yes', \"Int'l Plan:no\"): {'result': ['Churn?:False'],\n",
       "   'support': 0.23582358235823583,\n",
       "   'confidence': 0.946987951807229}},\n",
       " {('VMail Plan:no', 'CustServ Calls:1'): {'result': ['Churn?:False'],\n",
       "   'support': 0.21992199219921993,\n",
       "   'confidence': 0.8757467144563919}},\n",
       " {('CustServ Calls:1', 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.21992199219921993,\n",
       "   'confidence': 0.8757467144563919}},\n",
       " {(\"Int'l Plan:no\", 'CustServ Calls:1'): {'result': ['Churn?:False'],\n",
       "   'support': 0.29612961296129614,\n",
       "   'confidence': 0.9232927970065482}},\n",
       " {('VMail Plan:no', 'Area Code:415'): {'result': ['Churn?:False'],\n",
       "   'support': 0.2946294629462946,\n",
       "   'confidence': 0.8293918918918919}},\n",
       " {('Area Code:415', 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.2946294629462946,\n",
       "   'confidence': 0.8293918918918919}},\n",
       " {('Area Code:415', \"Int'l Plan:no\"): {'result': ['Churn?:False'],\n",
       "   'support': 0.39933993399339934,\n",
       "   'confidence': 0.8843853820598008}},\n",
       " {('VMail Plan:no', 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.6024602460246025,\n",
       "   'confidence': 0.8328494400663625}},\n",
       " {('VMail Plan:no', \"Int'l Plan:no\"): {'result': ['Churn?:False'],\n",
       "   'support': 0.5634563456345635,\n",
       "   'confidence': 0.861467889908257}},\n",
       " {(\"Int'l Plan:no\", 'VMail Message:0'): {'result': ['Churn?:False'],\n",
       "   'support': 0.5634563456345635,\n",
       "   'confidence': 0.861467889908257}},\n",
       " {('VMail Plan:no',\n",
       "   'CustServ Calls:1',\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.21992199219921993, 'confidence': 0.8757467144563919}},\n",
       " {('VMail Plan:no',\n",
       "   \"Int'l Plan:no\",\n",
       "   'CustServ Calls:1'): {'result': ['Churn?:False'], 'support': 0.20522052205220523, 'confidence': 0.9011857707509882}},\n",
       " {(\"Int'l Plan:no\",\n",
       "   'CustServ Calls:1',\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.20522052205220523, 'confidence': 0.9011857707509882}},\n",
       " {('Area Code:415',\n",
       "   'VMail Plan:no',\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.2946294629462946, 'confidence': 0.8293918918918919}},\n",
       " {('Area Code:415',\n",
       "   'VMail Plan:no',\n",
       "   \"Int'l Plan:no\"): {'result': ['Churn?:False'], 'support': 0.27782778277827785, 'confidence': 0.8558225508317931}},\n",
       " {('Area Code:415',\n",
       "   \"Int'l Plan:no\",\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.27782778277827785, 'confidence': 0.8558225508317931}},\n",
       " {('VMail Plan:no',\n",
       "   \"Int'l Plan:no\",\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.5634563456345635, 'confidence': 0.861467889908257}},\n",
       " {('VMail Plan:no',\n",
       "   \"Int'l Plan:no\",\n",
       "   'CustServ Calls:1',\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.20522052205220523, 'confidence': 0.9011857707509882}},\n",
       " {('Area Code:415',\n",
       "   'VMail Plan:no',\n",
       "   \"Int'l Plan:no\",\n",
       "   'VMail Message:0'): {'result': ['Churn?:False'], 'support': 0.27782778277827785, 'confidence': 0.8558225508317931}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules2.generateAllRules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vẫn không có yếu tố nào làm cho churn = True xảy ra mà làm cho yếu tố này thành phổ biến.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
